from typing import List
import random
import asyncio
import aiohttp
import os
import logging
from rich import print
from dotenv import load_dotenv
from aiohttp_socks import ProxyConnector, ProxyError
from config import _load_settings
from logs import logger


# Load environment variables
load_dotenv()

# Load logger settings from .env file
LOG_DIR_FETCHING = os.getenv("LOG_DIR_FETCHING")

# Create logger object
logger = logger.get_logger(log_file=LOG_DIR_FETCHING, log_level=logging.INFO)


class ResponseScraper:
    def __init__(self, start_page: int, end_page: int, category_id: int) -> None:
        """
        Initializes a new instance of the ResponseScraper class.

        Args:
            start_page (int): The starting page number for scraping.
            end_page (int): The ending page number for scraping.
            category_id (int): The category ID for scraping.

        Returns:
            None

        Initializes the instance variables `one_page_response`, `list_all_responses`, `start_page`,
        `end_page`, `category_id`, `__base_url_video`, `__base_url_page`, `__base_url_category`,
        `urls`, and `_user_agents` with the given values.

        The `urls` attribute is a list of URLs generated by combining the `__base_url_video`,
        `__base_url_page`, `page`, `__base_url_category`, and `category_id` attributes. The `page`
        variable ranges from `start_page` to `end_page + 1`.

        The `_user_agents` attribute is a list of user agents loaded from the scraping settings.
        """
        self.one_page_response: str = None
        self.list_all_responses: List = []
        self.start_page: int = start_page
        self.end_page: int = end_page
        self.category_id: int = category_id
        self.__base_url_video: str = _load_settings()['scraping_settings']['base_url_video']
        self.__base_url_page: str = _load_settings()['scraping_settings']['base_url_page']
        self.__base_url_category: str = _load_settings()['scraping_settings']['base_url_category']
        self.urls: List = [f'{self.__base_url_video}{self.__base_url_page}{page}{self.__base_url_category}{self.category_id}' for page in range(self.start_page, self.end_page + 1)]
        self._user_agents: List = _load_settings()['scraping_settings']['user_agents']

    async def _fetch(self, url: str, session: aiohttp.ClientSession):
        """
        Asynchronously fetches a web page from the given URL using the provided session and user agent.

        Args:
            url (str): The URL of the web page to fetch.
            session (aiohttp.ClientSession): The aiohttp client session to use for the request.

        Returns:
            str or None: The content of the fetched web page as a JSON string, or None if the request failed.

        Raises:
            ValueError: If the session parameter is None.

        Handles:
            ProxyError: Retries the request with an increased delay.
            aiohttp.ClientError, asyncio.TimeoutError, aiohttp.ClientPayloadError, aiohttp.ClientResponseError: Logs the error and returns None.
        """
        # Delay between requests
        delay: int = 1
        
        # If session is None, raise ValueError
        if session is None:
            raise ValueError("session parameter cannot be None")

        # Generate random user agent
        _headers: dict = {"User-Agent": random.choice(self._user_agents) if self._user_agents else None} # generate random user agent

        try:
            # Send GET request to the specified URL and get the response
            async with session.get(url=url, headers=_headers, timeout=aiohttp.ClientTimeout(total=30)) as response:
                await asyncio.sleep(random.uniform(0.5, 5.0))   # Add random delay between requests
                if response.status == 200:
                    logger.info(f"Request successful: {url} - {response.status}")
                    self.one_page_response: str = await response.json()
                    return self.one_page_response
                elif response.status == 429:
                    logger.warning(f"Too many requests: {url} - {response.status}")
                    await asyncio.sleep(60)  # Wait for 60 seconds before retrying
                    return await self._fetch(url, session)
                else:
                    logger.error(f"Request failed: {response.status}, message='{response.reason}', proxy_url={response.url}")
                    return None
        
        # Handle proxy errors 
        except ProxyError as e:
            logger.error(f"Error {e}, retrying in {delay} seconds...")
            await asyncio.sleep(delay)
            delay *= 2
        
        # Handle other errors
        except (aiohttp.ClientError, asyncio.TimeoutError, aiohttp.ClientPayloadError, aiohttp.ClientResponseError) as e:
            logger.error(f"Response request failed: {e}")
            return None


    async def _fetch_all_pages(self, working_proxies: List) -> List[str]:
        """
        Asynchronously fetches all pages from the given URLs using a random proxy from the working proxies list.

        Args:
            working_proxies (List[str]): A list of working proxies to use for fetching the pages.

        Returns:
            List[str]: A list of HTML responses from all fetched pages.
        """
        # List of tasks for each URL and gather the responses
        task_responses: List[asyncio.Task] = []

        if working_proxies:
            # Create a ProxyConnector with random proxy from proxy_list
            connector: ProxyConnector = ProxyConnector.from_url(random.choice(working_proxies))
        else:
            # ProxyConnector is None if there are no working proxies
            connector = None
        
        async with aiohttp.ClientSession(connector=connector) as session: # connector=connector
            # Create tasks for each URL and gather the responses
            for index, url in enumerate(self.urls):
                task_response_json_data: asyncio.Task = asyncio.create_task(self._fetch(url, session))
                task_responses.append(task_response_json_data)
            self.list_all_responses: List = await asyncio.gather(*task_responses)
        # Return the list of responses
        return self.list_all_responses
